{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1k8FsU5WZY9aUApJ-a2vLrkVMl2BJ6omf",
      "authorship_tag": "ABX9TyNX2D32oWeDzVZsO3bWztUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noranazmy/learnai/blob/main/QuranWordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring word proximities in the Quran\n",
        "\n",
        "This notebook is an exploration of the distribution of words in the Quran.\n",
        "\n",
        "Using a neural network, words are transformed into vectors where the distance between two vectors represents how often they appear in similar contexts. **Clusters of nodes represent where words tend to occur close to each other in the text, not what they mean or how they relate to each other conceptually.**\n",
        "\n",
        "The Quran is a text whose meaning and significance cannot be reduced to statistical patterns. These patterns cannot capture its spiritual depth or uncover any fundamental truths within it. On a personal note, I believe that **datafication is fundamentally opposite to the kind of engagement a text such as the Quran demands.**\n",
        "\n",
        "In fact, Arabic speakers will find this exercise to be a clear demonstration of the difficulty of applying this type of computation to Quranic Arabic, the richness that is immediately lost, and the sheer number of errors encountered as soon as you try to preprocess the individual words down to some normal form.\n",
        "\n",
        "For instance, attempting to reduce:\n",
        "* 'مالك'\n",
        "\n",
        "from the third verse of the Quran 'مالك يوم الدين'\n",
        "returns:\n",
        "\n",
        "* 'مال'\n",
        "\n",
        "when it should return: 'ملك'\n",
        "\n",
        "This exercise should not be considered as anything more than a technical challenge to:\n",
        "\n",
        "1. Work with neural networks\n",
        "1. Create and visualize word embeddings\n",
        "1. Work with the Arabic language and the Quran as a particularly challenging dataset, testing out different approaches to preprocessing and discovering the corresponding python libraries\n"
      ],
      "metadata": {
        "id": "_X7NLSR5R93I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "This notebook uses `camel-tools` to preprocess each word in the Quran down to a primary form such as a lemma or root. Particles like prepositions can also be removed."
      ],
      "metadata": {
        "id": "nMBMwW51Opz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Processing = \"Lemma\" # @param [\"Lemma\", \"Root\", \"None\"] {type:\"string\"}\n",
        "Particles = \"Exclude\" # @param [\"Include\", \"Exclude\"] {type:\"string\"}"
      ],
      "metadata": {
        "id": "jxS34WGIOTru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary packages\n",
        "!pip -q install fasttext camel-tools"
      ],
      "metadata": {
        "id": "2M8LLgffTbeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the morphology database\n",
        "!camel_data -i morphology-db-msa-r13"
      ],
      "metadata": {
        "id": "bHYEplhkK7YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.analyzer import Analyzer\n",
        "\n",
        "db = MorphologyDB.builtin_db()\n",
        "analyzer = Analyzer(db)\n",
        "\n",
        "def preprocess_token(token, form=\"lex\", skip_particles=False):\n",
        "    analyses = analyzer.analyze(token)\n",
        "    if not analyses:\n",
        "        return token\n",
        "\n",
        "    analysis = analyses[0]\n",
        "    pos = analysis.get('pos', '')\n",
        "\n",
        "    # Skip particles (prepositions, conjunctions, etc.)\n",
        "    if skip_particles and (pos in ['prep', 'conj', 'part']):\n",
        "        return None\n",
        "\n",
        "    if form in analysis and analysis[form].upper() != \"UNKNOWN\" and analysis[form].upper() != \"NTWS\":\n",
        "        return analysis[form]\n",
        "    return token\n",
        "\n",
        "def preprocess(verses):\n",
        "  if (Processing == \"None\"):\n",
        "    return verses\n",
        "  form = \"lex\" if Processing == \"Lemma\" else \"root\"\n",
        "  skip_particles = True if Particles == \"Exclude\" else False\n",
        "  processed_verses = []\n",
        "  for verse in verses:\n",
        "      tokens = verse.split()\n",
        "      roots = [preprocess_token(token, form, skip_particles) for token in tokens]\n",
        "      roots = [r for r in roots if r is not None]\n",
        "      if roots:\n",
        "          processed_verses.append(\" \".join(roots))\n",
        "  return processed_verses"
      ],
      "metadata": {
        "id": "4LSx8cG5J7G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the Quran corpus\n",
        "\n",
        "We use [Tanzil](http://tanzil.net/updates/) to download the entire Quran as text with diacritics, verse numbers, and other markers removed.\n",
        "\n",
        "* Tanzils Quran Text (Simple Clean, Version 1.1)\n",
        "* License: Creative Commons Attribution 3.0\n",
        "* Copyright (C) 2007-2025 Tanzil Project"
      ],
      "metadata": {
        "id": "0DFimiD0Pv1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYwMkKTOOn47"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Read the Quran file contents\n",
        "contents = Path('quran-simple-clean-no-verse-numbers.txt').read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Remove the copyright after the blank line\n",
        "quran_lines = contents.split(\"\\n\\n\", 1)[0]\n",
        "verses = quran_lines.split(\"\\n\")\n",
        "\n",
        "print(f\"Loaded {len(verses)} Quranic verses\")\n",
        "print(f\"{verses[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "We use CAMeL tools to replace each individual word with some normal form. This can be the **root** or **lemma** depending on user input. This step can also be fully disabled.\n",
        "\n",
        "Particles such as prepositions can also be removed."
      ],
      "metadata": {
        "id": "p2ydIm7biwot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform derived words into a primary form\n",
        "processed_verses = preprocess(verses)\n",
        "\n",
        "# Write the processed corpus temporarily with the copyright removed\n",
        "Path(\"quran-corpus.tmp\").write_text(\"\\n\".join(processed_verses), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Processing chosen: {Processing}.\")\n",
        "print(processed_verses[:15])"
      ],
      "metadata": {
        "id": "Hk_0pqTfMhxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "dftKKG-9kq5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "model = fasttext.train_unsupervised(\n",
        "    input=\"quran-corpus.tmp\",\n",
        "    model=\"skipgram\",\n",
        "    dim=100,\n",
        "    ws=5,\n",
        "    minn=1,\n",
        "    maxn=1,\n",
        "    epoch=25,\n",
        "    lr=0.05,\n",
        "    thread=2\n",
        ")\n",
        "\n",
        "vocabulary = set(model.get_words())\n",
        "vocabulary_size = len(model.get_words())\n",
        "print(f\"Finished training on {len(verses)} Quranic verses. Vocabulary size is {vocabulary_size}.\")"
      ],
      "metadata": {
        "id": "OT2_y9xeTqy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualization"
      ],
      "metadata": {
        "id": "M4rynvuekvKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# fastText doesn’t expose token counts directly\n",
        "token_counts = Counter()\n",
        "for verse in verses:\n",
        "    token_counts.update(verse.split())\n",
        "\n",
        "most_common = token_counts.most_common(vocabulary_size)\n",
        "export_words = [w for w, _ in most_common if w in vocabulary]\n",
        "pd.DataFrame(most_common, columns=[\"Token\", \"Frequency\"])"
      ],
      "metadata": {
        "id": "d4p9UwrSXYfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path(\"metadata.tsv\").write_text(\"\\n\".join(export_words) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "meta_lines = [\"token\\tfreq\"]\n",
        "for w in export_words:\n",
        "    meta_lines.append(f\"{w}\\t{token_counts[w]}\")\n",
        "Path(\"full_metadata.tsv\").write_text(\"\\n\".join(meta_lines) + \"\\n\", encoding=\"utf-8\")\n",
        "print(\"Wrote full_metadata.tsv\")"
      ],
      "metadata": {
        "id": "fSXfy4m2Z1YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vecs = np.vstack([model.get_word_vector(w) for w in export_words])\n",
        "\n",
        "with open(\"vectors.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in vecs:\n",
        "        f.write(\"\\t\".join(map(lambda x: f\"{x:.6f}\", row.tolist())) + \"\\n\")\n",
        "\n",
        "print(\"vectors.tsv shape:\", vecs.shape)\n"
      ],
      "metadata": {
        "id": "gOklrAcQa7KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "k = 15\n",
        "\n",
        "X = vecs.astype(np.float32)\n",
        "X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "neighbors = {}\n",
        "for i, w in enumerate(export_words):\n",
        "    sims = X @ X[i]                 # cosine sims to all\n",
        "    sims[i] = -1.0                  # exclude self\n",
        "    idx = np.argpartition(-sims, k)[:k]\n",
        "    idx = idx[np.argsort(-sims[idx])]\n",
        "    neighbors[w] = [{\"token\": export_words[j], \"score\": float(sims[j])} for j in idx]\n",
        "\n",
        "Path(\"neighbors.json\").write_text(json.dumps(neighbors, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(\"Wrote neighbors.json\")"
      ],
      "metadata": {
        "id": "vBElynA1bIa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r projector"
      ],
      "metadata": {
        "id": "J7VtSujeHktU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard.plugins import projector\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "LOG_DIR = \"projector\"\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# Copy metadata\n",
        "shutil.copy(\"metadata.tsv\", os.path.join(LOG_DIR, \"metadata.tsv\"))\n",
        "\n",
        "# Create checkpoint\n",
        "embedding_var = tf.Variable(vecs, name=\"embedding\")\n",
        "checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
        "checkpoint.save(os.path.join(LOG_DIR, \"embedding.ckpt\"))\n",
        "\n",
        "# Configure projector\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = \"metadata.tsv\"\n",
        "\n",
        "projector.visualize_embeddings(LOG_DIR, config)"
      ],
      "metadata": {
        "id": "Sh0Xl0uUbgKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir projector"
      ],
      "metadata": {
        "id": "G6f6nRHScC-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
